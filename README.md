```md
# LocalGPT: Sichere, lokale Unterhaltungen mit Ihren Dokumenten üåê

üö®üö® Sie k√∂nnen localGPT auf einer vorkonfigurierten [Virtual Machine](https://bit.ly/localGPT) ausf√ºhren. Vergewissern Sie sich, den Code PromptEngineering zu verwenden, um 50% Rabatt zu erhalten. Ich werde eine kleine Provision erhalten!

**LocalGPT** ist eine Open-Source-Initiative, die es Ihnen erm√∂glicht, mit Ihren Dokumenten zu interagieren, ohne Ihre Privatsph√§re zu gef√§hrden. Mit allem, was lokal l√§uft, k√∂nnen Sie sicher sein, dass keine Daten Ihren Computer verlassen. Tauchen Sie ein in die Welt der sicheren, lokalen Dokumenteninteraktionen mit LocalGPT.

## Funktionen üåü
- **H√∂chste Privatsph√§re**: Ihre Daten bleiben auf Ihrem Computer, was eine 100%ige Sicherheit gew√§hrleistet.
- **Vielseitige Modellunterst√ºtzung**: Integrieren Sie nahtlos eine Vielzahl von Open-Source-Modellen, darunter HF, GPTQ, GGML und GGUF.
- **Vielf√§ltige Einbettungen**: W√§hlen Sie aus einer Reihe von Open-Source-Einbettungen.
- **Verwenden Sie Ihr LLM erneut**: Nach dem Download k√∂nnen Sie Ihr LLM ohne wiederholte Downloads wiederverwenden.
- **Chatverlauf**: Erinnert sich an Ihre vorherigen Unterhaltungen (in einer Sitzung).
- **API**: LocalGPT verf√ºgt √ºber eine API, die Sie f√ºr den Aufbau von RAG-Anwendungen verwenden k√∂nnen.
- **Grafische Benutzeroberfl√§che**: LocalGPT wird mit zwei GUIs geliefert, eines verwendet die API und das andere ist eigenst√§ndig (basierend auf Streamlit).
- **GPU-, CPU- und MPS-Unterst√ºtzung**: Unterst√ºtzt mehrere Plattformen out-of-the-box. Unterhalten Sie sich mit Ihren Daten unter Verwendung von `CUDA`, `CPU` oder `MPS` und mehr!

## Tauchen Sie tiefer ein mit unseren Videos üé•
- [Detaillierte Code-Durchlauf](https://youtu.be/MlyoObdIHyo)
- [Llama-2 mit LocalGPT](https://youtu.be/lbFmceo4D5E)
- [Hinzuf√ºgen von Chatverlauf](https://youtu.be/d7otIM_MCZs)
- [LocalGPT - Aktualisiert (17.09.2023)](https://youtu.be/G_prHSKX9d4)

## Technische Details üõ†Ô∏è
Durch Auswahl der richtigen lokalen Modelle und der Leistung von `LangChain` k√∂nnen Sie die gesamte RAG-Pipeline lokal ausf√ºhren, ohne dass Daten Ihre Umgebung verlassen, und mit vern√ºnftiger Leistung.

- `ingest.py` verwendet `LangChain`-Tools, um das Dokument zu analysieren und lokal Einbettungen mit `InstructorEmbeddings` zu erstellen. Die Ergebnisse werden dann in einer lokalen Vektordatenbank mit `Chroma`-Vektorspeicher gespeichert.
- `run_localGPT.py` verwendet ein lokales LLM, um Fragen zu verstehen und Antworten zu erstellen. Der Kontext f√ºr die Antworten wird aus dem lokalen Vektorstore extrahiert, indem eine √Ñhnlichkeitssuche durchgef√ºhrt wird, um das richtige St√ºck Kontext aus den Dokumenten zu lokalisieren.
- Sie k√∂nnen dieses lokale LLM durch ein beliebiges anderes LLM von HuggingFace ersetzen. Stellen Sie sicher, dass das von Ihnen ausgew√§hlte LLM im HF-Format vorliegt.

Dieses Projekt wurde von dem urspr√ºnglichen [privateGPT](https://github.com/imartinez/privateGPT) inspiriert.

## Aufgebaut mit üß©
- [LangChain](https://github.com/hwchase17/langchain)
- [HuggingFace LLMs](https://huggingface.co/models)
- [InstructorEmbeddings](https://instructor-embedding.github.io/)
- [LLAMACPP](https://github.com/abetlen/llama-cpp-python)
- [ChromaDB](https://www.trychroma.com/)
- [Streamlit](https://streamlit.io/)

# Umgebung einrichten üåç

1. üì• Klonen Sie das Repository mit Git:

```shell
git clone https://github.com/PromtEngineer/localGPT.git
```

2. üêç Installieren Sie [conda](https://www.anaconda.com/download) f√ºr die Verwaltung von virtuellen Umgebungen. Erstellen und aktivieren Sie eine neue virtuelle Umgebung.

```shell
conda create -n localGPT python=3.10.0
conda activate localGPT
```

3. üõ†Ô∏è Installieren Sie die Abh√§ngigkeiten mit pip

Um Ihre Umgebung einzurichten, um den Code auszuf√ºhren, installieren Sie zun√§chst alle Anforderungen:

```shell
pip install -r requirements.txt
```

***LLAMA-CPP installieren:***

LocalGPT verwendet [LlamaCpp-Python](https://github.com/abetlen/llama-cpp-python) f√ºr GGML (Sie ben√∂tigen llama-cpp-python <=0.1.76) und GGUF-Modelle (llama-cpp-python >=0.1.83).

Wenn Sie BLAS oder Metal mit [llama-cpp](https://github.com/abetlen/llama-cpp-python#installation-with-openblas--cublas--clblast--metal) verwenden m√∂chten, k√∂nnen Sie entsprechende Flags setzen:

F√ºr `NVIDIA`-GPU-Unterst√ºtzung, verwenden Sie `cuBLAS`

```shell
# Beispiel: cuBLAS
CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.83 --no-cache-dir
```

F√ºr Apple Metal (`M1/M2`) Unterst√ºtzung, verwenden Sie

```shell
# Beispiel: METAL
CMAKE_ARGS="-DLLAMA_METAL=on"  FORCE_CMAKE=1 pip install llama-cpp-python==0.1.83 --no-cache-dir
```
F√ºr weitere Details, siehe [llama-cpp](https://github.com/abetlen/llama-cpp-python#installation-with-openblas--cublas--clblast--metal)

## Docker üê≥

Die Installation der erforderlichen Pakete f√ºr die GPU-Inferenz auf NVIDIA-GPUs wie gcc 11 und CUDA 11 kann Konflikte mit anderen Paketen in Ihrem System verursachen.
Als Alternative zu Conda k√∂nnen Sie Docker mit der bereitgestellten Dockerdatei verwenden.
Es enth√§lt CUDA, Ihr System ben√∂tigt lediglich Docker, BuildKit, Ihren NVIDIA-GPU-Treiber und das NVIDIA-Container-Toolkit.
Erstellen

 Sie mit `docker build -t localgpt .`, BuildKit ist erforderlich.
Docker BuildKit unterst√ºtzt derzeit keine GPU w√§hrend der *docker build*-Zeit, nur w√§hrend der *docker run*-Zeit.
F√ºhren Sie mit `docker run -it --mount src="$HOME/.cache",target=/root/.cache,type=bind --gpus=all localgpt` aus.

## Testdatensatz

Zu Testzwecken wird dieses Repository mit der [Verfassung der USA](https://constitutioncenter.org/media/files/constitution.pdf) als Beispieldatei mitgeliefert.

## Importieren Ihrer EIGENEN Daten.
Legen Sie Ihre Dateien in den Ordner `SOURCE_DOCUMENTS`. Sie k√∂nnen mehrere Ordner innerhalb des Ordners `SOURCE_DOCUMENTS` platzieren, und der Code wird Ihre Dateien rekursiv lesen.

### Unterst√ºtzte Dateiformate:
LocalGPT unterst√ºtzt derzeit die folgenden Dateiformate. LocalGPT verwendet `LangChain` zum Laden dieser Dateiformate. Der Code in `constants.py` verwendet ein `DOCUMENT_MAP`-W√∂rterbuch, um ein Dateiformat auf den entsprechenden Loader zuzuordnen. Um ein anderes Dateiformat zu unterst√ºtzen, f√ºgen Sie einfach dieses W√∂rterbuch mit dem Dateiformat und dem entsprechenden Loader aus [LangChain](https://python.langchain.com/docs/modules/data_connection/document_loaders/) hinzu.

```shell
DOCUMENT_MAP = {
    ".txt": TextLoader,
    ".md": TextLoader,
    ".py": TextLoader,
    ".pdf": PDFMinerLoader,
    ".csv": CSVLoader,
    ".xls": UnstructuredExcelLoader,
    ".xlsx": UnstructuredExcelLoader,
    ".docx": Docx2txtLoader,
    ".doc": Docx2txtLoader,
}
```

### Eingestellt

F√ºhren Sie den folgenden Befehl aus, um alle Daten einzulesen.

Wenn Sie `cuda` auf Ihrem System eingerichtet haben.

```shell
python ingest.py
```
Sie sehen eine Ausgabe wie diese:
<img width="1110" alt="Screenshot 2023-09-14 at 3 36 27 PM" src="https://github.com/PromtEngineer/localGPT/assets/134474669/c9274e9a-842c-49b9-8d95-606c3d80011f">


Verwenden Sie das Ger√§tetypargument, um ein bestimmtes Ger√§t anzugeben.
Zum Ausf√ºhren auf `cpu`

```sh
python ingest.py --device_type cpu
```

Um auf `M1/M2` auszuf√ºhren

```sh
python ingest.py --device_type mps
```

Verwenden Sie die Hilfe f√ºr eine vollst√§ndige Liste der unterst√ºtzten Ger√§te.

```sh
python ingest.py --help
```

Dies erstellt einen neuen Ordner namens `DB` und verwendet ihn f√ºr den neu erstellten Vektorstore. Sie k√∂nnen so viele Dokumente einlesen, wie Sie m√∂chten, und alle werden in der lokalen Einbettungsdatenbank akkumuliert.
Wenn Sie von einer leeren Datenbank aus starten m√∂chten, l√∂schen Sie `DB` und lesen Sie Ihre Dokumente erneut ein.

Hinweis: Wenn Sie dies zum ersten Mal ausf√ºhren, ben√∂tigt es Internetzugriff, um das Einbettungsmodell herunterzuladen (Standard: `Instructor Embedding`). Bei den nachfolgenden Ausf√ºhrungen verl√§sst keine Daten Ihre lokale Umgebung, und Sie k√∂nnen Daten ohne Internetverbindung einlesen.

## Stellen Sie Fragen an Ihre Dokumente, lokal!

Um mit Ihren Dokumenten zu chatten, f√ºhren Sie den folgenden Befehl aus (standardm√§√üig wird es auf `cuda` ausgef√ºhrt).

```shell
python run_localGPT.py
```
Sie k√∂nnen auch den Ger√§tetyp angeben, genau wie bei `ingest.py`

```shell
python run_localGPT.py --device_type mps # um auf Apple-Silizium auszuf√ºhren
```

Dies l√§dt den eingelesenen Vektorstore und das Einbettungsmodell. Sie erhalten eine Aufforderung:

```shell
> Geben Sie eine Abfrage ein:
```

Nachdem Sie Ihre Frage eingegeben haben, dr√ºcken Sie die Eingabetaste. LocalGPT ben√∂tigt einige Zeit basierend auf Ihrer Hardware. Sie erhalten eine Antwort wie unten dargestellt.
<img width="1312" alt="Screenshot 2023-09-14 at 3 33 19 PM" src="https://github.com/PromtEngineer/localGPT/assets/134474669/a7268de9-ade0-420b-a00b-ed12207dbe41">

Sobald die Antwort generiert wurde, k√∂nnen Sie eine weitere Frage stellen, ohne das Skript erneut auszuf√ºhren. Warten Sie einfach auf die erneute Aufforderung.

***Hinweis:*** Wenn Sie dies zum ersten Mal ausf√ºhren, ben√∂tigt es eine Internetverbindung, um das LLM herunterzuladen (Standard: `TheBloke/Llama-2-7b-Chat-GGUF`). Danach k√∂nnen Sie Ihre Internetverbindung trennen, und die Skriptinferenz funktioniert trotzdem. Keine Daten verlassen Ihre lokale Umgebung.

Geben Sie `exit` ein, um das Skript zu beenden.

### Zus√§tzliche Optionen mit run_localGPT.py

Sie k√∂nnen das Flag `--show_sources` mit `run_localGPT.py` verwenden, um anzuzeigen, welche Abschnitte vom Einbettungsmodell abgerufen wurden. Standardm√§√üig werden 4 verschiedene Quellen/Abschnitte angezeigt. Sie k√∂nnen die Anzahl der Quellen/Abschnitte √§ndern

```shell
python run_localGPT.py --show_sources
```

Eine andere Option besteht darin, den Chatverlauf zu aktivieren. ***Hinweis***: Dies ist standardm√§√üig deaktiviert und kann mit dem Flag `--use_history` aktiviert werden. Das Kontextfenster ist begrenzt, daher verwendet das Aktivieren von History es und kann √ºberlaufen.

```shell
python run_localGPT.py --use_history
```

Sie k√∂nnen Benutzerfragen und Modellantworten mit dem Flag `--save_qa` in eine csv-Datei `/local_chat_history/qa_log.csv` speichern. Jede Interaktion wird gespeichert.

```shell
python run_localGPT.py --save_qa
```

# F√ºhren Sie die grafische Benutzeroberfl√§che aus

1. √ñffnen Sie `constants.py` in einem Editor Ihrer Wahl, und je nach Auswahl f√ºgen Sie das LLM hinzu, das Sie verwenden m√∂chten. Standardm√§√üig wird das folgende Modell verwendet:

   ```shell
   MODEL_ID = "TheBloke/Llama-2-7b-Chat-GGUF"
   MODEL_BASENAME = "llama-2-

7b-chat.Q4_K_M.gguf"
   ```

3. √ñffnen Sie ein Terminal und aktivieren Sie Ihre Python-Umgebung, die die Abh√§ngigkeiten aus requirements.txt installiert hat.

4. Navigieren Sie zum Verzeichnis `/LOCALGPT`.

5. F√ºhren Sie den folgenden Befehl aus: `python run_localGPT_API.py`. Die API sollte gestartet werden.

6. Warten Sie, bis alles geladen ist. Sie sollten etwas wie `INFO:werkzeug:Press CTRL+C to quit.` sehen.

7. √ñffnen Sie ein zweites Terminal und aktivieren Sie dieselbe Python-Umgebung.

8. Navigieren Sie zum Verzeichnis `/LOCALGPT/localGPTUI`.

9. F√ºhren Sie den Befehl `python localGPTUI.py` aus.

10. √ñffnen Sie einen Webbrowser und gehen Sie zur Adresse `http://localhost:5111/`.


# Wie w√§hlt man verschiedene LLM-Modelle aus?

Um die Modelle zu √§ndern, m√ºssen sowohl `MODEL_ID` als auch `MODEL_BASENAME` festgelegt werden.

1. √ñffnen Sie `constants.py` in einem Editor Ihrer Wahl.
2. √Ñndern Sie die `MODEL_ID` und `MODEL_BASENAME`. Wenn Sie ein quantisiertes Modell (`GGML`, `GPTQ`, `GGUF`) verwenden, m√ºssen Sie `MODEL_BASENAME` angeben. F√ºr unquantisierte Modelle setzen Sie `MODEL_BASENAME` auf `NONE`
5. Es gibt eine Reihe von Beispielmodellen von HuggingFace, die bereits getestet wurden, um mit dem original trainierten Modell verwendet zu werden (enden mit HF oder haben eine .bin in ihren "Files and versions"), und quantisierte Modelle (enden mit GPTQ oder haben eine .no-act-order oder .safetensors in ihren "Files and versions").
6. F√ºr Modelle, die mit HF enden oder eine .bin in ihren "Files and versions" auf ihrer HuggingFace-Seite haben.

   - Stellen Sie sicher, dass Sie eine `MODEL_ID` ausgew√§hlt haben. Zum Beispiel -> `MODEL_ID = "TheBloke/guanaco-7B-HF"`
   - Gehen Sie zum [HuggingFace Repo](https://huggingface.co/TheBloke/guanaco-7B-HF)

7. F√ºr Modelle, die GPTQ in ihrem Namen enthalten und/oder eine .no-act-order oder .safetensors-Erweiterung in ihren "Files and versions" auf ihrer HuggingFace-Seite haben.

   - Stellen Sie sicher, dass Sie eine `MODEL_ID` ausgew√§hlt haben. Zum Beispiel -> model_id = `"TheBloke/wizardLM-7B-GPTQ"`
   - Gehen Sie zum entsprechenden [HuggingFace Repo](https://huggingface.co/TheBloke/wizardLM-7B-GPTQ) und w√§hlen Sie "Files and versions".
   - W√§hlen Sie einen der Modellnamen aus und setzen Sie ihn als `MODEL_BASENAME`. Zum Beispiel -> `MODEL_BASENAME = "wizardLM-7B-GPTQ-4bit.compat.no-act-order.safetensors"`

8. Befolgen Sie die gleichen Schritte f√ºr `GGUF`- und `GGML`-Modelle.

# GPU- und VRAM-Anforderungen

Nachfolgend finden Sie die VRAM-Anforderung f√ºr verschiedene Modelle, abh√§ngig von ihrer Gr√∂√üe (Milliarden von Parametern). Die Sch√§tzungen in der Tabelle enthalten nicht den VRAM, der von den Einbettungsmodellen verwendet wird - diese verwenden zus√§tzlich 2 GB-7 GB VRAM, abh√§ngig vom Modell.

| Modellgr√∂√üe (B) | float32   | float16   | GPTQ 8bit      | GPTQ 4bit          |
| ------- | --------- | --------- | -------------- | ------------------ |
| 7B      | 28 GB     | 14 GB     | 7 GB - 9 GB    | 3,5 GB - 5 GB      |
| 13B     | 52 GB     | 26 GB     | 13 GB - 15 GB  | 6,5 GB - 8 GB      |
| 32B     | 130 GB    | 65 GB     | 32,5 GB - 35 GB| 16,25 GB - 19 GB   |
| 65B     | 260,8 GB  | 130,4 GB  | 65,2 GB - 67 GB| 32,6 GB - 35 GB    |


# Systemanforderungen

## Python-Version

Um diese Software verwenden zu k√∂nnen, muss Python 3.10 oder h√∂her installiert sein. Fr√ºhere Versionen von Python werden nicht kompiliert.

## C++-Compiler

Wenn Sie beim Erstellen eines Rades w√§hrend des `pip install`-Vorgangs einen Fehler erhalten, m√ºssen Sie m√∂glicherweise einen C++-Compiler auf Ihrem Computer installieren.

### F√ºr Windows 10/11

Um einen C++-Compiler unter Windows 10/11 zu installieren, befolgen Sie diese Schritte:

1. Installieren Sie Visual Studio 2022.
2. Stellen Sie sicher,

 dass w√§hrend der Installation die Workload "Desktopentwicklung mit C++" ausgew√§hlt ist.
3. Aktivieren Sie in der Workload-Einstellung "Desktopentwicklung mit C++" das Kontrollk√§stchen "C++-CMake-Tools f√ºr Windows" unter "Einzelne Komponenten".
4. Starten Sie Ihren Computer neu, nachdem Sie die Installation abgeschlossen haben.

### F√ºr macOS

macOS verf√ºgt standardm√§√üig √ºber den C++-Compiler Clang. Sie sollten keine zus√§tzlichen Schritte ausf√ºhren m√ºssen.

### F√ºr Linux

Unter Linux k√∂nnen Sie den C++-Compiler GCC installieren, indem Sie den folgenden Befehl in Ihrem Terminal ausf√ºhren:

```bash
sudo apt-get update
sudo apt-get install build-essential
```

## Numpy-Abh√§ngigkeit

Stellen Sie sicher, dass Sie das Numpy-Paket installiert haben. Wenn Sie Numpy nicht haben, k√∂nnen Sie es mit dem folgenden Befehl installieren:

```shell
pip install numpy
```

# Beitrag

LocalGPT ist ein Open-Source-Projekt und wir begr√º√üen Beitr√§ge von der Community! F√ºhlen Sie sich frei, [ein Issue zu √∂ffnen](https://github.com/PromtEngineer/localGPT/issues), einen Vorschlag zu machen oder einen Pull-Request zu senden.

# Lizenz

LocalGPT ist unter der [MIT-Lizenz](https://github.com/PromtEngineer/localGPT/blob/main/LICENSE) lizenziert.
```

Ich hoffe, diese Anleitung ist hilfreich f√ºr Sie! Wenn Sie weitere Fragen haben oder Unterst√ºtzung ben√∂tigen, lassen Sie es mich wissen!
